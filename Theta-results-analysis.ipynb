{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9f3b8d",
   "metadata": {},
   "source": [
    "# HEPnOS Autotuning result analysis\n",
    "\n",
    "This notebook analyzes the results obtained by running DeepHyper with the HEPnOS event selection workflow on the Theta supercomputer. These experiments consist of 115 individual job, each running for 1h on 128 nodes.\n",
    "\n",
    "## Structure of the results\n",
    "\n",
    "Each job produced a CSV file containing the list of configurations evaluated, as well as objective achieved by each configuration, and additional information such as the timestamps at which the configuration started running and stopped running, and the nodes on which it ran. The objective corresponds to `-log(runtime)` of the configuration.\n",
    "\n",
    "Results files are named using the following convention:\n",
    "- `exp-<MODEL>-<SCALE>-<ENABLE_PEP>-<MORE_PARAMS>-<ITERATION>.csv` for non-transfer-learning experiments;\n",
    "- `exp-tl-<MODEL>-<SCALE>-<ENABLE_PEP>-<MORE_PARAMS>-<ITERATION>.csv` for non-transfer-learning experiments.\n",
    "\n",
    "Where `<MODEL>` is either `RF` (random forest), `GP` (gaussian process) or `DUMMY` (random sampling), `<SCALE>` is the number of nodes used by each HEPnOS workflow instance (4, 8, or 16), `<ENABLE_PEP>` indicates whether the second part of the workflow (event selection step) is enabled (`true` or `false`), `<MORE_PARAMS>` indicates whether a reduced set of parameters is used (`false`) or the full set (`true`), finally as each experiment was reproduced 5 times, the `<ITERATION>` is the iteration number of the experiment.\n",
    "\n",
    "Experiments were done as follows. For each model, we first run with 4 nodes per instance, without the event selection step, and with a reduced parameter set (`exp-<MODEL>-4-false-false`). We then add the event selection step (`exp-<MODEL>-4-true-false`), then more parameters (`exp-<MODEL>-4-true-true`). Finally we scale to 8 nodes per instance (`exp-<MODEL>-8-true-true`) then 16 (`exp-<MODEL>-16-true-true`). It results that some scenarios were not experimented with (such as 8 or 16 nodes without the event selection, or more parameters without event selection).\n",
    "\n",
    "For transfer learning (applicable to only the `RF` and `GP` models, since `DUMMY` only consists of random sampling), we base the transfer learning from the previous corresponding job without transfer learning, as follow.\n",
    "- `exp-tl-<MODEL>-16-true-true-<X>` uses `exp-<MODEL>-8-true-true`;\n",
    "- `exp-tl-<MODEL>-8-true-true` uses `exp-<MODEL>-4-true-true`;\n",
    "- `exp-tl-<MODEL>-4-true-true` uses `exp-<MODEL>-4-true-false`;\n",
    "- `exp-tl-<MODEL>-4-true-false` uses `exp-<MODEL>-4-false-false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dc3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the list of CSV files we will be analyzing\n",
    "!ls results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c0cc2",
   "metadata": {},
   "source": [
    "## Parameter space\n",
    "\n",
    "In this section, we list the parameters that make up our parameter space, along with their range, distribution, and descriptions.\n",
    "\n",
    "| Workflow part | Parameter | Range | Distribution | Description |\n",
    "|---|---|---|---|---|\n",
    "| All | BusySpin | True/False | Uniform | Whether Mercury should busy-spin instead of blocking on epoll |\n",
    "| HEPnOS | ProgressThread | True/False | Uniform | Whether to use a dedicated network progress thread in HEPnOS servers |\n",
    "|  | NumRPCthreads | [0, 63] | Uniform | Number of threads used by HEPnOs servers to service RPC |\n",
    "|  | NumEventDBs | [1, 16] | Uniform | Number of database instances per HEPnOS server for Events |\n",
    "|  | NumProductDBs | [1, 16] | Uniform | Number of database instances per HEPnOS server for Products |\n",
    "|  | NumProviders | [1, 32] | Uniform | Number of database providers per HEPnOS server |\n",
    "|  | ThreadPoolType | {fifo, fifo_wait, prio_wait} | Uniform | Argobots thread pool type each provider uses |\n",
    "|  | PESperNode | {1, 2, 4, 8, 16, 32} | Uniform | Number of HEPnOS server processes per physical node |\n",
    "| Dataloader | ProgressThread | True/False | Uniform | Whether to use a dedicated network progress thread in Dataloader processes |\n",
    "|  | WriteBatchSize | [1, 2048] | Log-uniform | Size of the batches (in number of events used when sending data to HEPnOS |\n",
    "|  | PESperNode | {1, 2, 4, 8, 16, 32} | Uniform | Number of Dataloader processes per physical node |\n",
    "| PEP | ProgressThread | True/False | Uniform | Whether to use a dedicated network progress thread in PEP processes |\n",
    "|  | NumThreads | [1, 31] | Uniform | Uniform & Number of threads use to process data in parallel |\n",
    "|  | InputBatchSize | [8, 1024] | Log-uniform | Batch size (in number of events) to use when loading data from HEPnOS |\n",
    "|  | OuputBatchSize | [8, 1024] | Log-uniform | Batch size (in number of events) to use when sending data across PEP processes |\n",
    "|  | PESperNode | {1, 2, 4, 8, 16, 32} | Uniform | Number of PEP processes per physical node |\n",
    "| Dataloader (cont'd) | LoaderAsync | True/False | Uniform | Use threads to asynchronously send batches to HEPnOS |\n",
    "|  | LoaderAsyncThreads | [1, 63] | Log-uniform | Number of threads for asynchronous store in Dataloader |\n",
    "| PEP (cont'd) | UsePreloading | True/False | Uniform | Use batch-prefetching of data products instead of per-product load |\n",
    "|  | UseRDMA | True/False | Uniform | Use RDMA to transfer data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053d396",
   "metadata": {},
   "source": [
    "## Some python definitions\n",
    "\n",
    "The bellow code provides utility classes and functions to load the experiments' CSV files, plot them, an compute various quantities. This code is not interesting in itself. For the analysis of the results, please jum to [the analysis section](#analysis) (after having run the cells in this section, of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14daecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from functools import cache\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a Job class representing a single repetition of an experiment\n",
    "\n",
    "class Job:\n",
    "    \n",
    "    def __init__(self, model: str, scale: int, enable_pep: bool, more_params: bool, iteration: int, tl=False):\n",
    "        assert model in ['DUMMY', 'RF', 'GP']\n",
    "        assert scale in [4, 8, 16]\n",
    "        assert iteration in [1, 2, 3, 4, 5]\n",
    "        if more_params:\n",
    "            assert enable_pep\n",
    "        self.__model = model\n",
    "        self.__scale = scale\n",
    "        self.__enable_pep = enable_pep\n",
    "        self.__more_params = more_params\n",
    "        self.__iteration = iteration\n",
    "        self.__tl = tl\n",
    "    \n",
    "    @property\n",
    "    def filename(self):\n",
    "        \"\"\"Returns the name of the CSV file corresponding to this job.\"\"\"\n",
    "        prefix = 'exp-tl' if self.__tl else 'exp'\n",
    "        enable_pep = str(self.__enable_pep).lower()\n",
    "        more_params = str(self.__more_params).lower()\n",
    "        return f'results/{prefix}-{self.__model}-{self.__scale}-{enable_pep}-{more_params}-{self.__iteration}.csv'\n",
    "    \n",
    "    @cache\n",
    "    def load_csv_to_dataframe(self, *, only_timestamps: bool =False):\n",
    "        \"\"\"Loads the CSV file of this job and returns a dataframe.\n",
    "        Only the relevant information (objective and timestamp_end,\n",
    "        and timestamp_start) are left in the dataframe.\n",
    "        The objective is transformed back into a runtime = exp(-objective).\"\"\"\n",
    "        filename = self.filename\n",
    "        if only_timestamps:\n",
    "            selection = ['timestamp_end', 'timestamp_start']\n",
    "        else:\n",
    "            selection = ['objective', 'timestamp_end', 'timestamp_start']\n",
    "        df = pd.read_csv(filename)[selection]\n",
    "        # convert timestamp to minutes\n",
    "        df['timestamp_end'] = df['timestamp_end'].apply(lambda t: t/60)\n",
    "        df['timestamp_start'] = df['timestamp_start'].apply(lambda t: t/60)\n",
    "        if not only_timestamps:\n",
    "            # the objective is expressed as a -log(time), so transform them into a time\n",
    "            df['objective'] = pd.to_numeric(df['objective'], errors='coerce')\n",
    "            df.dropna(inplace=True)\n",
    "            df['objective'] = df['objective'].apply(lambda x: math.exp(-x))\n",
    "            # compute the running best for this series\n",
    "            df['best'] = df['objective'].cummin()\n",
    "            df.sort_values('timestamp_end', inplace=True)\n",
    "        return df\n",
    "    \n",
    "    @cache\n",
    "    def count_instances(self):\n",
    "        \"\"\"Loads the CSV file of this job and counts the failures and timeouts.\"\"\"\n",
    "        filename = self.filename\n",
    "        objective = pd.read_csv(filename)['objective']\n",
    "        failures = list(objective.loc[pd.to_numeric(objective, errors='coerce').isnull()])\n",
    "        total = len(objective)\n",
    "        success = total - len(failures)\n",
    "        return {'timeout': failures.count('F10001'),\n",
    "                'failure': failures.count('F10002') + failures.count('F10003'),\n",
    "                'total': total,\n",
    "                'success': success}\n",
    "    \n",
    "    def plot_scatter(self, ax = None, color: str = 'b'):\n",
    "        \"\"\"Draws a scatter plot of this job. Each point represents an evaluation.\"\"\"\n",
    "        df = self.load_csv_to_dataframe()\n",
    "        ax = df.plot(x='timestamp_end', y='objective', kind='scatter',\n",
    "                xlabel='search time (minutes)', ylabel='run time (sec)',\n",
    "                ax=ax, fontsize=16, color=color, logy=True,\n",
    "                yticks=[10,20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600])\n",
    "        ax.grid(axis='y')\n",
    "        ax.xaxis.label.set_size(18)\n",
    "        ax.yaxis.label.set_size(18)\n",
    "        return ax\n",
    "        \n",
    "    def plot_progress(self, ax = None, color: str = 'b'):\n",
    "        \"\"\"Plots the evolution of the run-time of the best found configuration\n",
    "        as time progresses.\"\"\"\n",
    "        df = self.load_csv_to_dataframe()\n",
    "        ax = df.plot(x='timestamp_end', y='best',\n",
    "            xlabel='search time (minutes)', ylabel='best config (sec)',\n",
    "            ax=ax, fontsize=16, color=color)\n",
    "        ax.xaxis.label.set_size(18)\n",
    "        ax.yaxis.label.set_size(18)\n",
    "        ax.legend(fontsize=18)\n",
    "        return ax\n",
    "    \n",
    "    @cache\n",
    "    def compute_utilization(self):\n",
    "        \"\"\"Compute the number of running instances (percentage of a max)\n",
    "        as a function of time, and returns the corresponding dataframe.\"\"\"\n",
    "        df = self.load_csv_to_dataframe(only_timestamps=True)\n",
    "        history = []\n",
    "        for _, row in df.iterrows():\n",
    "            history.append((row['timestamp_start'], 1))\n",
    "            history.append((row['timestamp_end'], -1))\n",
    "        history = sorted(history, key=lambda v: v[0])\n",
    "        nb_instances = 0\n",
    "        timestamp = [0]\n",
    "        n_instances_running = [0]\n",
    "        max_nb_instances = 128 / self.__scale\n",
    "        for time, incr in history:\n",
    "            nb_instances += incr\n",
    "            timestamp.append(time)\n",
    "            n_instances_running.append(nb_instances*100.0/max_nb_instances)\n",
    "        return pd.DataFrame({'time': timestamp, 'jobs': n_instances_running})\n",
    "    \n",
    "    def plot_utilization(self, ax = None, color: str = 'b'):\n",
    "        \"\"\"Plots the number of running instances as a function of time.\"\"\"\n",
    "        df = self.compute_utilization()\n",
    "        ax = df.plot(x='time', y='jobs',\n",
    "            xlabel='search time (minutes)', ylabel='worker utilization (%)',\n",
    "            ax=ax, fontsize=16, color=color)\n",
    "        ax.xaxis.label.set_size(18)\n",
    "        ax.yaxis.label.set_size(18)\n",
    "        ax.set_ylim(ymin=0, ymax=100)\n",
    "        ax.legend(['instances'], fontsize=18)\n",
    "        return ax\n",
    "    \n",
    "    def compute_average_utilization(self):\n",
    "        df = self.compute_utilization()\n",
    "        df = df.reset_index()\n",
    "        s = 0.0\n",
    "        t_prev = 0.0\n",
    "        for index, row in df.iterrows():\n",
    "            t = row['time']\n",
    "            u = row['jobs']\n",
    "            dt = t - t_prev\n",
    "            s += dt * u\n",
    "            t_prev = t\n",
    "        s /= t\n",
    "        return s\n",
    "    \n",
    "    def compute_best_objective(self):\n",
    "        df = self.load_csv_to_dataframe()\n",
    "        return df['best'].iloc[-1]\n",
    "        \n",
    "    def compute_average_best_objective(self):\n",
    "        df = self.load_csv_to_dataframe()\n",
    "        df = df.reset_index()\n",
    "        s = 0.0\n",
    "        o_prev = None\n",
    "        t_prev = 0.0\n",
    "        for index, row in df.iterrows():\n",
    "            t = row['timestamp_end']\n",
    "            o = row['best']\n",
    "            dt = t - t_prev\n",
    "            if o_prev is not None:\n",
    "                s += dt * o_prev\n",
    "            else:\n",
    "                s += dt * o\n",
    "            o_prev = o\n",
    "            t_prev = t\n",
    "        s /= t\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d416de",
   "metadata": {},
   "source": [
    "Let's test this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = Job('RF', 4, True, True, 4)\n",
    "job.plot_scatter()\n",
    "job.plot_progress()\n",
    "job.plot_utilization()\n",
    "print(f'number of instances = {job.count_instances()}')\n",
    "print(f'average utilization = {job.compute_average_utilization()}')\n",
    "print(f'average best objective = {job.compute_average_best_objective()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40268d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a class representing an experiment (5 jobs with the same parameters)\n",
    "\n",
    "class Exp:\n",
    "    \n",
    "    def __init__(self, model: str, scale: int, enable_pep: bool, more_params: bool, tl=False):\n",
    "        self.__jobs = [Job(model, scale, enable_pep, more_params, i, tl) for i in range(1, 6)]\n",
    "        \n",
    "    @property\n",
    "    def jobs(self):\n",
    "        return self.__jobs\n",
    "    \n",
    "    def count_instances(self):\n",
    "        result = {'failure': 0, 'timeout': 0, 'total': 0, 'success': 0}\n",
    "        _min = None\n",
    "        _max = 0\n",
    "        for job in self.__jobs:\n",
    "            f = job.count_instances()\n",
    "            result['failure'] += f['failure']\n",
    "            result['timeout'] += f['timeout']\n",
    "            result['total'] += f['total']\n",
    "            result['success'] += f['success']\n",
    "            if _min is None or _min > f['total']:\n",
    "                _min = f['total']\n",
    "            if _max < f['total']:\n",
    "                _max = f['total']\n",
    "        result['min'] = _min\n",
    "        result['max'] = _max\n",
    "        result['average'] = result['total'] / len(self.__jobs)\n",
    "        return result\n",
    "        \n",
    "    @cache\n",
    "    def build_aggregated_dataframe(self):\n",
    "        \"\"\"Builds an aggregated dataframe from the 5 jobs of the experiment.\n",
    "        This dataframe will gave all the timestamps in its time column, and\n",
    "        run_1,...5 will contain the best known configuration for each job at\n",
    "        each timestamp.\n",
    "        The min, max, and mean column correspond to the minimum, maximum, and\n",
    "        mean of the run_* columns.\"\"\"\n",
    "        dfs = [job.load_csv_to_dataframe().copy() for job in self.__jobs]\n",
    "        aggregated_df = None\n",
    "        for i, df in enumerate(dfs):\n",
    "            df.rename(columns={'best': f'run_{i+1}'}, inplace=True)\n",
    "            df.drop('objective', axis='columns', inplace=True)\n",
    "            df.drop('timestamp_start', axis='columns', inplace=True)\n",
    "            if aggregated_df is None:\n",
    "                aggregated_df = df\n",
    "            else:\n",
    "                aggregated_df = pd.merge_ordered(\n",
    "                    aggregated_df, df, on='timestamp_end', fill_method='ffill')\n",
    "        aggregated_df.fillna(method='backfill', axis=0, inplace=True)\n",
    "        col_names = ['run_1', 'run_2', 'run_3', 'run_4', 'run_5']\n",
    "        runs_df = aggregated_df[col_names]\n",
    "        aggregated_df['min'] = runs_df.min(axis=1)\n",
    "        aggregated_df['max'] = runs_df.max(axis=1)\n",
    "        aggregated_df['mean'] = runs_df.mean(axis=1)\n",
    "        return aggregated_df\n",
    "    \n",
    "    def plot_progress(self, ax = None, color: str = 'b', legend: str = 'mean'):\n",
    "        \"\"\"Plot the progress of the experiment using the min/max/mean of\n",
    "        the aggregated dataframe as a function of time.\"\"\"\n",
    "        df = self.build_aggregated_dataframe()\n",
    "        ax = df.plot(x='timestamp_end', y=['mean'], ax=ax, color=color,\n",
    "                     xlabel='search time (minutes)', ylabel='best config (sec)', fontsize=16)\n",
    "        ax.fill_between(x=df['timestamp_end'], y1=df['min'], y2=df['max'], alpha=0.3,\n",
    "                        color=color)\n",
    "        if legend:\n",
    "            ax.legend([legend, 'min/max'], fontsize=16)\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "        ax.xaxis.label.set_size(18)\n",
    "        ax.yaxis.label.set_size(18)\n",
    "        return ax\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_multiple(exp1, exp2, ax = None, colors = ['blue', 'red'],\n",
    "                      legends = None):\n",
    "        \"\"\"Plots two experiments in the same figure.\"\"\"\n",
    "        assert len(colors) == 2\n",
    "        assert legends is None or len(legends) == 2\n",
    "        dfs = [exp1.build_aggregated_dataframe(),\n",
    "               exp2.build_aggregated_dataframe()]\n",
    "        for i, df in enumerate(dfs):\n",
    "            ax = df.plot(x='timestamp_end', y=['mean'], ax=ax, color=colors[i],\n",
    "                         xlabel='search time (minutes)', ylabel='best config (sec)', fontsize=16)\n",
    "            ax.fill_between(x=df['timestamp_end'], y1=df['min'], y2=df['max'], alpha=0.3,\n",
    "                            color=colors[i])\n",
    "        if legends:\n",
    "            l = []\n",
    "            for legend in legends:\n",
    "                l.append(legend)\n",
    "                l.append('min/max') # legend for the min/max area\n",
    "            ax.legend(l, fontsize=16)\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "        ax.xaxis.label.set_size(18)\n",
    "        ax.yaxis.label.set_size(18)\n",
    "        return ax\n",
    "            \n",
    "    @staticmethod\n",
    "    def __cleanup_dataframes(exp, exp_tl):\n",
    "        \"\"\"Helper function to build, cleanup, and merge the aggregated\n",
    "        dataframes of two experiments. It will return a single dataframe\n",
    "        with a 'time' column, and a mean-no-tl and mean-tl columns,\n",
    "        extracting the mean from each experiment respectively.\"\"\"\n",
    "        # get time series\n",
    "        df = exp.build_aggregated_dataframe().copy()\n",
    "        df_tl = exp_tl.build_aggregated_dataframe().copy()\n",
    "        # drop unnecessary columns\n",
    "        for col_name in [f'run_{i}' for i in range(1,6)] + ['min', 'max']:\n",
    "            df.drop(col_name, axis='columns', inplace=True)\n",
    "            df_tl.drop(col_name, axis='columns', inplace=True)\n",
    "        # rename columns\n",
    "        df.rename(columns={'mean': 'mean-no-tl'}, inplace=True)\n",
    "        df_tl.rename(columns={'mean': 'mean-tl'}, inplace=True)\n",
    "        # merge the dataframes\n",
    "        merged_df = pd.merge_ordered(\n",
    "            df, df_tl, on='timestamp_end', fill_method='ffill')\n",
    "        # backfill NaN\n",
    "        merged_df.fillna(method='backfill', axis=0, inplace=True)\n",
    "        return merged_df\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_expected_improvement_factor(exp, exp_tl):\n",
    "        \"\"\"Compute the expected improvement factor between an experiment\n",
    "        with TL and an experiment without (see paper for definition).\"\"\"\n",
    "        merged_df = Exp.__cleanup_dataframes(exp, exp_tl)\n",
    "        # compute time deltas\n",
    "        merged_df['dt'] = merged_df['timestamp_end'].diff()\n",
    "        merged_df['dt'][0] = 0.0\n",
    "        # compute instantaneous improvement factor F(t)dt\n",
    "        merged_df['Fdt'] = merged_df['dt'] * merged_df['mean-no-tl'] / merged_df['mean-tl']\n",
    "        integral = merged_df['Fdt'].sum()\n",
    "        expected_improvement = integral / (merged_df['timestamp_end'].iloc[-1] - merged_df['timestamp_end'][0])\n",
    "        return expected_improvement\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_expected_speedup(exp, exp_tl, dt: float = 0.0001):\n",
    "        \"\"\"Compute the expected speedup between an experiment with TL\n",
    "        and an experiment without (see paper for definition).\"\"\"\n",
    "        df = Exp.__cleanup_dataframes(exp, exp_tl)\n",
    "        t_first = df['mean-no-tl'][0]\n",
    "        t_min = df['mean-no-tl'].iloc[-1]\n",
    "        num = (t_first - t_min) / dt\n",
    "        idx_no_tl = 0\n",
    "        idx_tl = 0\n",
    "        mean_no_tl = df['mean-no-tl']\n",
    "        mean_tl = df['mean-tl']\n",
    "        L = len(mean_tl)\n",
    "        T = 0.0\n",
    "        for t in np.linspace(t_min, t_first, num=int(num)):\n",
    "            while mean_no_tl[idx_no_tl] > t and idx_no_tl < L-1:\n",
    "                idx_no_tl += 1\n",
    "            while mean_tl[idx_tl] > t and idx_no_tl < L-1:\n",
    "                idx_tl += 1\n",
    "            T_no_tl = df['timestamp_end'][idx_no_tl]\n",
    "            T_tl = df['timestamp_end'][idx_tl]\n",
    "            T += T_no_tl/T_tl\n",
    "        T /= num\n",
    "        return T\n",
    "    \n",
    "    def compute_average_utilization(self):\n",
    "        utilization = [j.compute_average_utilization() for j in self.jobs]\n",
    "        return {\n",
    "            'average': sum(utilization) / len(self.jobs),\n",
    "            'min': min(utilization),\n",
    "            'max': max(utilization)\n",
    "        }\n",
    "    \n",
    "    def compute_average_best_objective(self):\n",
    "        avg_best_objectives = [j.compute_average_best_objective() for j in self.jobs]\n",
    "        return {\n",
    "            'average': sum(avg_best_objectives) / len(self.jobs),\n",
    "            'min': min(avg_best_objectives),\n",
    "            'max': max(avg_best_objectives)\n",
    "        }\n",
    "    \n",
    "    def compute_best_objective(self):\n",
    "        best_objectives = [j.compute_best_objective() for j in self.jobs]\n",
    "        return {\n",
    "            'average': sum(best_objectives) / len(self.jobs),\n",
    "            'min': min(best_objectives),\n",
    "            'max': max(best_objectives)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test this class\n",
    "exp = Exp('DUMMY', 4, False, False)\n",
    "exp.plot_progress(color='orange')\n",
    "exp.count_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test plottingt two experiments\n",
    "exp1 = Exp('RF', 4, True, True)\n",
    "exp2 = Exp('RF', 4, True, True, tl=True)\n",
    "plot = Exp.plot_multiple(exp1, exp2, legends=['without TL', 'with TL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7256f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp.compute_expected_improvement_factor(exp1, exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d422403",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp.compute_expected_speedup(exp1, exp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1.compute_average_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to load all of the results in a dictionary\n",
    "def load_all_the_results():\n",
    "    exps = {}\n",
    "    for model in ['DUMMY', 'RF', 'GP']:\n",
    "        exps[f'{model}-4-F-F'] = Exp(model, 4, False, False)\n",
    "        exps[f'{model}-4-T-F'] = Exp(model, 4, True, False)\n",
    "        for scale in [4, 8, 16]:\n",
    "            exps[f'{model}-{scale}-T-T'] = Exp(model, scale, True, True)\n",
    "    for model in ['RF', 'GP']:\n",
    "        exps[f'TL-{model}-4-T-F'] = Exp(model, 4, True, False, tl=True)\n",
    "        for scale in [4, 8, 16]:\n",
    "            exps[f'TL-{model}-{scale}-T-T'] = Exp(model, scale, True, True, tl=True)\n",
    "    return exps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d7e500",
   "metadata": {},
   "source": [
    "## Plotting and analyzing the results\n",
    "\n",
    "<a id='analysis'></a>\n",
    "\n",
    "In this section we will analyze the results. Please follow the comments to play around with the results and plot various graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bafe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = load_all_the_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the expected speedup and improvement factor for all the RF experiments\n",
    "rf_exp_names = [exp_name for exp_name in exps if exp_name.startswith('RF-')]\n",
    "for exp_name in rf_exp_names:\n",
    "    tl_exp_name = f'TL-{exp_name}'\n",
    "    if tl_exp_name not in exps:\n",
    "        continue\n",
    "    exp, tl_exp = exps[exp_name], exps[tl_exp_name]\n",
    "    f = Exp.compute_expected_improvement_factor(exp, tl_exp)\n",
    "    s = Exp.compute_expected_speedup(exp, tl_exp)\n",
    "    print(f'{exp_name}:\\t E[F] = {f}\\t E[S] = {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3e36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot all the figures\n",
    "\n",
    "if not os.path.isdir('figures'):\n",
    "    os.mkdir('figures')\n",
    "\n",
    "# non-TL experiments\n",
    "for exp_name, exp in exps.items():\n",
    "    if exp_name.startswith('TL-'):\n",
    "        continue\n",
    "    # plot average and min/max progress out of the 5 repetitions\n",
    "    print(f'plotting {exp_name}-progress.pdf...')\n",
    "    exp.plot_progress().get_figure().savefig(\n",
    "        f'figures/{exp_name}-progress.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # plot each job's scatter plot and worker utilization\n",
    "    for i, job in enumerate(exp.jobs):\n",
    "        print(f'plotting {exp_name}-{i}-scatter.pdf...')\n",
    "        job.plot_scatter().get_figure().savefig(\n",
    "            f'figures/{exp_name}-{i}-scatter.pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'plotting {exp_name}-{i}-workers.pdf...')\n",
    "        job.plot_utilization().get_figure().savefig(\n",
    "            f'figures/{exp_name}-{i}-workers.pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "# TL experiments (plot against their non-TL counterparts)\n",
    "for tl_exp_name, tl_exp in exps.items():\n",
    "    if not tl_exp_name.startswith('TL-'):\n",
    "        continue\n",
    "    non_tl_exp_name = tl_exp_name[3:]\n",
    "    non_tl_exp = exps[non_tl_exp_name]\n",
    "    print(f'plotting {tl_exp_name}-best.pdf...')\n",
    "    Exp.plot_multiple(non_tl_exp, tl_exp, legends=['without TL', 'with TL']).get_figure().savefig(\n",
    "        f'figures/{tl_exp_name}-best.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    # plot each job's scatter plot and worker utilization\n",
    "    for i, (tl_job, non_tl_job) in enumerate(zip(tl_exp.jobs, non_tl_exp.jobs)):\n",
    "        print(f'plotting {tl_exp_name}-{i}-scatter.pdf...')\n",
    "        ax = non_tl_job.plot_scatter(color='blue')\n",
    "        ax = tl_job.plot_scatter(ax=ax, color='red')\n",
    "        ax.get_figure().savefig(\n",
    "            f'figures/{tl_exp_name}-{i}-scatter.pdf', bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f'plotting {tl_exp_name}-{i}-workers.pdf...')\n",
    "        ax = tl_job.plot_utilization().get_figure().savefig(\n",
    "            f'figures/{tl_exp_name}-{i}-workers.pdf', bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bffc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute some quantities to plot some bar charts now\n",
    "\n",
    "index = [\"4-F-F\", \"4-T-F\", \"4-T-T\", \"8-T-T\", \"16-T-T\"]\n",
    "models = ['DUMMY', 'RF', 'GP', 'TL-RF', 'TL-GP']\n",
    "data = {}\n",
    "\n",
    "for model in models:\n",
    "    data[model] = {\n",
    "        'evaluations': [],\n",
    "        'evaluations min': [],\n",
    "        'evaluations max': [],\n",
    "        'utilization': [],\n",
    "        'utilization min': [],\n",
    "        'utilization max': [],\n",
    "        'average objective': [],\n",
    "        'average objective min': [],\n",
    "        'average objective max': [],\n",
    "        'best objective': [],\n",
    "        'best objective min': [],\n",
    "        'best objective max': []\n",
    "    }\n",
    "    for suffix in index:\n",
    "        exp_name = f'{model}-{suffix}'\n",
    "        if exp_name not in exps:\n",
    "            for array_name in data[model]:\n",
    "                data[model][array_name].append(0.0)\n",
    "            continue\n",
    "        exp = exps[exp_name]\n",
    "        evaluations = exp.count_instances()\n",
    "        data[model]['evaluations'].append(evaluations['average'])\n",
    "        data[model]['evaluations min'].append(evaluations['min'])\n",
    "        data[model]['evaluations max'].append(evaluations['max'])\n",
    "        utilization = exp.compute_average_utilization()\n",
    "        data[model]['utilization'].append(utilization['average'])\n",
    "        data[model]['utilization min'].append(utilization['min'])\n",
    "        data[model]['utilization max'].append(utilization['max'])\n",
    "        average_best_objective = exp.compute_average_best_objective()\n",
    "        data[model]['average objective'].append(average_best_objective['average'])\n",
    "        data[model]['average objective min'].append(average_best_objective['min'])\n",
    "        data[model]['average objective max'].append(average_best_objective['max'])\n",
    "        overall_best_objective = exp.compute_best_objective()\n",
    "        data[model]['best objective'].append(overall_best_objective['average'])\n",
    "        data[model]['best objective min'].append(overall_best_objective['min'])\n",
    "        data[model]['best objective max'].append(overall_best_objective['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the number of evaluations as a bar chart\n",
    "evaluations = pd.DataFrame(\n",
    "    {model : data[model]['evaluations'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "means = evaluations\n",
    "mins = pd.DataFrame(\n",
    "    {model : data[model]['evaluations min'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "maxs = pd.DataFrame(\n",
    "    {model : data[model]['evaluations max'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in evaluations.columns]\n",
    "\n",
    "ax = evaluations.plot.bar(rot=0, color=['green', 'red', 'blue', 'orange', 'cyan'], fontsize=16,\n",
    "                          ylabel='Number of evaluations', xlabel='Experiment type',\n",
    "                          yerr=errors, width=0.8)\n",
    "ax.xaxis.label.set_size(18)\n",
    "ax.yaxis.label.set_size(18)\n",
    "ax.legend(fontsize=16)\n",
    "ax.get_figure().savefig(f'figures/bar-chart-evaluations.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ab0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the average worker utilization as a bar chart\n",
    "utilization = pd.DataFrame(\n",
    "    {model : data[model]['utilization'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "means = utilization\n",
    "mins = pd.DataFrame(\n",
    "    {model : data[model]['utilization min'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "maxs = pd.DataFrame(\n",
    "    {model : data[model]['utilization max'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in utilization.columns]\n",
    "\n",
    "ax = utilization.plot.bar(rot=0, color=['green', 'red', 'blue', 'orange', 'cyan'], fontsize=16,\n",
    "                          ylabel='Worker utilization (%)', xlabel='Experiment type',\n",
    "                          yerr=errors, width=0.8)\n",
    "ax.xaxis.label.set_size(18)\n",
    "ax.yaxis.label.set_size(18)\n",
    "ax.legend(loc='lower right')\n",
    "ax.get_figure().savefig(f'figures/bar-chart-utilization.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee68dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the average best objective as a bar chart\n",
    "objective = pd.DataFrame(\n",
    "    {model : data[model]['average objective'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "means = objective\n",
    "mins = pd.DataFrame(\n",
    "    {model : data[model]['average objective min'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "maxs = pd.DataFrame(\n",
    "    {model : data[model]['average objective max'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in objective.columns]\n",
    "\n",
    "ax = objective.plot.bar(rot=0, color=['green', 'red', 'blue', 'orange', 'cyan'], fontsize=16,\n",
    "                        ylabel='Average best (sec)', xlabel='Experiment type',\n",
    "                        yerr=errors, width=0.8)\n",
    "ax.xaxis.label.set_size(18)\n",
    "ax.yaxis.label.set_size(18)\n",
    "ax.legend(fontsize=16)\n",
    "ax.get_figure().savefig(f'figures/bar-chart-average-best.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961a329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the overall best objective as a bar chart\n",
    "best = pd.DataFrame(\n",
    "    {model : data[model]['best objective'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "means = best\n",
    "mins = pd.DataFrame(\n",
    "    {model : data[model]['best objective min'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "maxs = pd.DataFrame(\n",
    "    {model : data[model]['best objective max'] for model in models},\n",
    "    index=index\n",
    ")\n",
    "errors = [[means[c] - mins[c], maxs[c] - means[c]] for c in best.columns]\n",
    "\n",
    "ax = best.plot.bar(rot=0, color=['green', 'red', 'blue', 'orange', 'cyan'], fontsize=16,\n",
    "                   ylabel='Best configuration (sec)', xlabel='Experiment type',\n",
    "                   yerr=errors, width=0.8)\n",
    "ax.xaxis.label.set_size(18)\n",
    "ax.yaxis.label.set_size(18)\n",
    "ax.legend(fontsize=16)\n",
    "ax.get_figure().savefig(f'figures/bar-chart-overall-best.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a176d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
